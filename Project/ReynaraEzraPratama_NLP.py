# -*- coding: utf-8 -*-
"""Project_NLP_Using_Tensorflow.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1D0EACPsr6KtfriJp4M8cGz8dIb5jX9o3

# **NLP Model Using Tensorflow**
## **Reynara Ezra Pratama**
"""

import tensorflow as tf
print(tf.__version__)

# Commented out IPython magic to ensure Python compatibility.
!pip install ipython-autotime
# %load_ext autotime

"""# **Get Data**"""

import pandas as pd

df = pd.read_csv('/content/emotion.txt', names=['sentence', 'emote'], sep=';')

"""# **Check Data**"""

df.head()

print('Number of Data:',len(df))

df['emote'].value_counts()

"""# **Create New Dataframe**"""

category = pd.get_dummies(df['emote'])
category.head()

df = df.drop('emote', axis=1)

df_new = pd.concat([df, category], axis=1)
df_new.head()

text = df_new['sentence'].values
emote = df_new[['anger', 'fear', 'joy', 'love', 'sadness', 'surprise']].values

text

emote

"""# **Split Data**"""

from sklearn.model_selection import train_test_split

text_train, text_test, emote_train, emote_test = train_test_split(
    text,
    emote,
    test_size=0.2,
    random_state=0
)

"""# **Tokenization**"""

from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

tokenizer = Tokenizer(num_words=20000, oov_token='-')
tokenizer.fit_on_texts(text_train)
tokenizer.fit_on_texts(text_test)

sequence_train = tokenizer.texts_to_sequences(text_train)
sequence_test = tokenizer.texts_to_sequences(text_test)

pad_train = pad_sequences(sequence_train)
pad_test = pad_sequences(sequence_test)

"""# **Callback**"""

reduce_LR = tf.keras.callbacks.ReduceLROnPlateau(
    monitor="val_loss",
    factor=0.15,
    patience=5,
    min_lr=2.e-5
)

stop_early = tf.keras.callbacks.EarlyStopping(
    monitor="val_loss",
    min_delta=0,
    patience=10,
    verbose=0,
    mode="auto",
    baseline=None,
    restore_best_weights=True
)

"""# **Model**"""

model = tf.keras.models.Sequential([
   tf.keras.layers.Embedding(input_dim=20000, output_dim=64),
   tf.keras.layers.Dropout(0.4),
   tf.keras.layers.LSTM(64),
   tf.keras.layers.Dropout(0.3),
   tf.keras.layers.Dense(64, activation='relu'),
   tf.keras.layers.Dense(6, activation='softmax')
])

model.summary()

"""# **Run Model**"""

model.compile(
    optimizer='Adam',
    loss = 'categorical_crossentropy',
    metrics = ['accuracy']
)

history = model.fit(
    pad_train,
    emote_train,
    epochs = 100,
    callbacks = [reduce_LR, stop_early],
    validation_data = (pad_test, emote_test),
    verbose = 1
)

"""# **Plot Model History**"""

import matplotlib.pyplot as plt

def plot_accuracy(history):
  plt.figure(figsize=(18,5))
  acc = history.history['accuracy']
  val_acc = history.history['val_accuracy']
  epochs = range(len(acc))
  plot_acc = plt.plot(epochs, acc, 'red', label='Training Accuracy')
  plot_val_acc = plt.plot(epochs, val_acc, 'blue', label='Validation Accuracy')
  plt.xlabel('Epoch', fontsize=15)
  plt.ylabel('Accuracy', fontsize=15)
  plt.title('Training and Validation Accuracy', fontsize=25)
  plt.legend(bbox_to_anchor=(1,1), loc='best')
  plt.grid()
  plt.show()

def plot_loss(history):
  plt.figure(figsize=(18,5))
  loss = history.history['loss']
  val_loss = history.history['val_loss']
  epochs = range(len(loss))
  plot_loss = plt.plot(epochs, loss, 'red', label='Training Loss')
  plot_val_loss = plt.plot(epochs, val_loss, 'blue', label='Validation Loss')
  plt.xlabel('Epoch', fontsize=15)
  plt.ylabel('Loss', fontsize=15)
  plt.title('Training and Validation Loss', fontsize=25)
  plt.legend(bbox_to_anchor=(1,1), loc='best')
  plt.grid()
  plt.show()

plot_accuracy(history)

plot_loss(history)